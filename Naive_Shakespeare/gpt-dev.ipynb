{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import hashlib\n",
    "import os\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is 50257\n"
     ]
    }
   ],
   "source": [
    "blobpath = \"https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/vocab.bpe\"\n",
    "cache_key = hashlib.sha1(blobpath.encode()).hexdigest()\n",
    "tiktoken_cache_dir = \"./tiktoken_cache\"\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = tiktoken_cache_dir\n",
    "assert os.path.exists(os.path.join(tiktoken_cache_dir, cache_key))\n",
    "\n",
    "# Now you can use tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(f\"The size of the vocabulary is {enc.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 4178, 612]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "encode = enc.encode\n",
    "def decode(input_data):\n",
    "    # Check if input is a PyTorch tensor\n",
    "    if isinstance(input_data, torch.Tensor):\n",
    "        input_data = input_data.tolist()\n",
    "        print(input_data)\n",
    "    return enc.decode(input_data)\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([338025]) torch.int64\n",
      "tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([64, 8]) tensor([[ 1225,   713,   290, 32363,   257,   582,   338,    30],\n",
      "        [17903,   422, 11906,  6002,  1272,    11,   198,    39],\n",
      "        [  534, 33558,   284,   262,   640,    13,   198,  7120],\n",
      "        [ 2538, 35830,  1546,    25,   198,   198,    44,  2390],\n",
      "        [  300,  6315,    25,   198,    44,  6532,    11,   467],\n",
      "        [ 3963, 14545,  4944,    51,    25,   198,  2514,   307],\n",
      "        [  502,    11,   290,   777, 11906,  9730,    11,   198],\n",
      "        [ 2767,    25,   198,    46,  1793,     0,   750, 43989],\n",
      "        [19337,    11,  5609,  2415,     0,   198,  2437,   783],\n",
      "        [ 2937,    25,   198,    40,  1833, 17903,   880,    26],\n",
      "        [   13,  1649,    11,   327,  1872,   385,    11, 10598],\n",
      "        [20889,   329,    26, 26509, 11906,  3956,   198,  3886],\n",
      "        [  198,   464, 12296,   286,   534, 34685,    26, 10598],\n",
      "        [  262, 17435,    26,   356,   389,  2677,  8616,   338],\n",
      "        [ 2390,  8267,    40,  2937,    25,   198,    35,    86],\n",
      "        [  606,  8181,    13,   198,   198,    32,  3208,  1173],\n",
      "        [  198,  8048,   351,   683,    11,   866,   351,   683],\n",
      "        [  198,   198,    35,    52,  7336,  3963, 28154,    25],\n",
      "        [  198,  8763,  2606,    34,  1546,  5781,    25,   198],\n",
      "        [   12,  2339, 48388,   286, 11906,  5238,    11,   198],\n",
      "        [  287,   428,  1842,    11,   345,  1842,   534,  1200],\n",
      "        [  616,  1918,   351,   474, 20706,   510,   290,   866],\n",
      "        [ 2107,    11,   198,  6385,   262,   517,  3148,   290],\n",
      "        [ 1336,   880,    11,   198,   817,   280,   284,   502],\n",
      "        [ 1002,   345,    11, 31693,   641,   952,    11,   198],\n",
      "        [   26,   198,  1870,  1918,    11,   407, 43989,    11],\n",
      "        [   39,   451,   338,    83, 14210,    11,  8706,    30],\n",
      "        [  508,  6151,   683,   198,   818,   257,   749, 13674],\n",
      "        [  455,    11,   257,  1573,   351,   345,     0,   198],\n",
      "        [ 2873,    25,   198,  2348,   441,    11,  1521,   716],\n",
      "        [  422,   262, 11083,   286,  1475,  2357,    11,   198],\n",
      "        [   11,   314,   716,   530,   326,    11,  5149,  2081],\n",
      "        [49828,    32,    25,   198,  2949,    11,   588,   257],\n",
      "        [   45,   323,    11,  1282,    11,   314, 12472, 17903],\n",
      "        [  198,   198,  2538, 35830,  1546,    25,   198,  2953],\n",
      "        [14210, 14999,    11,   198,    43,  3263,   356,   743],\n",
      "        [   11,   484,  1975,   340,    26,   290,   351,   282],\n",
      "        [ 5210,    11,  1223,   198, 18820,  5210,    26,   198],\n",
      "        [   11,   198, 18947,   994,   287,  9538,   290,   743],\n",
      "        [19878,    34,    56,    25,   198,  5297,    11,   616],\n",
      "        [  502,    25,   198, 10248, 22817,   307,   616,  1545],\n",
      "        [  466, 15290, 17903,  4145,    13,   198,  1273,   696],\n",
      "        [  304,   260,   616, 10329,  4485,   422,   262,  4534],\n",
      "        [  262, 26951,    11,   290,   673, 26614,   340,   198],\n",
      "        [  288,  1229,  1381,    25,  1309,   502,   423,   198],\n",
      "        [  198,    40,   466,  7284,  1453,   354,   345,    11],\n",
      "        [   25,  4249,   373,   470,   881,    11,   198,   817],\n",
      "        [   26,   198,    56,   623,  1067, 15970,  2236,   407],\n",
      "        [  318,  3991,   198,   818, 11906,  2612,    12, 18041],\n",
      "        [22027,   329,  2032,  1211,   284,  1842,    11,   290],\n",
      "        [   11,   477,  1115,   466,  1826,   198,   818, 17903],\n",
      "        [16933, 47538,    11,   198,  1532,  1683,   597,  1036],\n",
      "        [ 3549,  1044,    13,   198,   198,  5097, 29267, 12532],\n",
      "        [ 2437,   673,  6622,   510,   262, 45508,    11,   262],\n",
      "        [  198, 39276,  2042, 42208,   504,    11, 28448,    11],\n",
      "        [  339,  2872,  1549,  1864,   284,   465,  1181,    11],\n",
      "        [ 3478, 18639,   319,   262, 14110,   431,   666,  3881],\n",
      "        [  991,   588, 11906,   944,    11,   198,  1870,  1650],\n",
      "        [  307,  3280,  1549,    13,   360,   455, 14210,  3285],\n",
      "        [ 5195,    11, 14210, 14880, 37713,   290, 14274, 12154],\n",
      "        [ 1842,   287,  3958, 17862,    26,   198, 21991,  1239],\n",
      "        [   13,  3914,   683,   307,   475,  8844,   261,   798],\n",
      "        [ 2726,  1517,    11,   198,  1870,   783,  3421,   284],\n",
      "        [27938,  1037,    11,  1865,   466,   407,   198,  4933]])\n",
      "targets: torch.Size([64, 8]) tensor([[  713,   290, 32363,   257,   582,   338,    30,   198],\n",
      "        [  422, 11906,  6002,  1272,    11,   198,    39,   776],\n",
      "        [33558,   284,   262,   640,    13,   198,  7120,  5891],\n",
      "        [35830,  1546,    25,   198,   198,    44,  2390,  8267],\n",
      "        [ 6315,    25,   198,    44,  6532,    11,   467,   878],\n",
      "        [14545,  4944,    51,    25,   198,  2514,   307,   257],\n",
      "        [   11,   290,   777, 11906,  9730,    11,   198,  2396],\n",
      "        [   25,   198,    46,  1793,     0,   750, 43989,   338],\n",
      "        [   11,  5609,  2415,     0,   198,  2437,   783,     0],\n",
      "        [   25,   198,    40,  1833, 17903,   880,    26,   290],\n",
      "        [ 1649,    11,   327,  1872,   385,    11, 10598,   318],\n",
      "        [  329,    26, 26509, 11906,  3956,   198,  3886, 39127],\n",
      "        [  464, 12296,   286,   534, 34685,    26, 10598,  1276],\n",
      "        [17435,    26,   356,   389,  2677,  8616,   338,  2460],\n",
      "        [ 8267,    40,  2937,    25,   198,    35,    86,  2120],\n",
      "        [ 8181,    13,   198,   198,    32,  3208,  1173,   666],\n",
      "        [ 8048,   351,   683,    11,   866,   351,   683,     0],\n",
      "        [  198,    35,    52,  7336,  3963, 28154,    25,   198],\n",
      "        [ 8763,  2606,    34,  1546,  5781,    25,   198,    40],\n",
      "        [ 2339, 48388,   286, 11906,  5238,    11,   198,   817],\n",
      "        [  428,  1842,    11,   345,  1842,   534,  1200,   523],\n",
      "        [ 1918,   351,   474, 20706,   510,   290,   866,     0],\n",
      "        [   11,   198,  6385,   262,   517,  3148,   290, 15121],\n",
      "        [  880,    11,   198,   817,   280,   284,   502, 11906],\n",
      "        [  345,    11, 31693,   641,   952,    11,   198,  5574],\n",
      "        [  198,  1870,  1918,    11,   407, 43989,    11,  1011],\n",
      "        [  451,   338,    83, 14210,    11,  8706,    30,   198],\n",
      "        [ 6151,   683,   198,   818,   257,   749, 13674,  1948],\n",
      "        [   11,   257,  1573,   351,   345,     0,   198,   198],\n",
      "        [   25,   198,  2348,   441,    11,  1521,   716,   314],\n",
      "        [  262, 11083,   286,  1475,  2357,    11,   198,  6653],\n",
      "        [  314,   716,   530,   326,    11,  5149,  2081,   739],\n",
      "        [   32,    25,   198,  2949,    11,   588,   257,  3331],\n",
      "        [  323,    11,  1282,    11,   314, 12472, 17903,    11],\n",
      "        [  198,  2538, 35830,  1546,    25,   198,  2953,   262],\n",
      "        [14999,    11,   198,    43,  3263,   356,   743,    11],\n",
      "        [  484,  1975,   340,    26,   290,   351,   282,   483],\n",
      "        [   11,  1223,   198, 18820,  5210,    26,   198,  1639],\n",
      "        [  198, 18947,   994,   287,  9538,   290,   743,   804],\n",
      "        [   34,    56,    25,   198,  5297,    11,   616,   922],\n",
      "        [   25,   198, 10248, 22817,   307,   616,  1545,    11],\n",
      "        [15290, 17903,  4145,    13,   198,  1273,   696,    11],\n",
      "        [  260,   616, 10329,  4485,   422,   262,  4534,   338],\n",
      "        [26951,    11,   290,   673, 26614,   340,   198,   261],\n",
      "        [ 1229,  1381,    25,  1309,   502,   423,   198,    32],\n",
      "        [   40,   466,  7284,  1453,   354,   345,    11,  1011],\n",
      "        [ 4249,   373,   470,   881,    11,   198,   817,   280],\n",
      "        [  198,    56,   623,  1067, 15970,  2236,   407,  3285],\n",
      "        [ 3991,   198,   818, 11906,  2612,    12, 18041,    11],\n",
      "        [  329,  2032,  1211,   284,  1842,    11,   290,   287],\n",
      "        [  477,  1115,   466,  1826,   198,   818, 17903,   379],\n",
      "        [47538,    11,   198,  1532,  1683,   597,  1036, 12587],\n",
      "        [ 1044,    13,   198,   198,  5097, 29267, 12532,    25],\n",
      "        [  673,  6622,   510,   262, 45508,    11,   262,  2855],\n",
      "        [39276,  2042, 42208,   504,    11, 28448,    11,   290],\n",
      "        [ 2872,  1549,  1864,   284,   465,  1181,    11,   198],\n",
      "        [18639,   319,   262, 14110,   431,   666,  3881,    11],\n",
      "        [  588, 11906,   944,    11,   198,  1870,  1650, 17903],\n",
      "        [ 3280,  1549,    13,   360,   455, 14210,  3285,    11],\n",
      "        [   11, 14210, 14880, 37713,   290, 14274, 12154,  1549],\n",
      "        [  287,  3958, 17862,    26,   198, 21991,  1239,   804],\n",
      "        [ 3914,   683,   307,   475,  8844,   261,   798,   287],\n",
      "        [ 1517,    11,   198,  1870,   783,  3421,   284,   705],\n",
      "        [ 1037,    11,  1865,   466,   407,   198,  4933,    65]])\n"
     ]
    }
   ],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.block_size]\n",
    "        y = self.data[idx+1:idx+self.block_size+1]\n",
    "        return x, y\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "block_size = 8\n",
    "batch_size = 64\n",
    "# Create datasets\n",
    "train_dataset = SequenceDataset(train_data, block_size)\n",
    "val_dataset = SequenceDataset(val_data, block_size)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Usage example\n",
    "for xb, yb in train_loader:\n",
    "    print('inputs:', xb.shape, xb)\n",
    "    print('targets:', yb.shape, yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is:\n",
      " [1225] the target:\n",
      " [713]\n",
      "when input is:\n",
      " [1225, 713] the target:\n",
      " [290]\n",
      "when input is:\n",
      " [1225, 713, 290] the target:\n",
      " [32363]\n",
      "when input is:\n",
      " [1225, 713, 290, 32363] the target:\n",
      " [257]\n",
      "when input is:\n",
      " [1225, 713, 290, 32363, 257] the target:\n",
      " [582]\n",
      "when input is:\n",
      " [1225, 713, 290, 32363, 257, 582] the target:\n",
      " [338]\n",
      "when input is:\n",
      " [1225, 713, 290, 32363, 257, 582, 338] the target:\n",
      " [30]\n",
      "when input is:\n",
      " [1225, 713, 290, 32363, 257, 582, 338, 30] the target:\n",
      " [198]\n"
     ]
    }
   ],
   "source": [
    "for t in range(block_size):\n",
    "    context = xb[0][:t+1].tolist()  # Convert to list\n",
    "    target = [yb[0][t].item()]      # Convert to a single-element list\n",
    "    print(f\"when input is:\\n {context} the target:\\n {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        return logits  # return raw logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        old = idx\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(old)\n",
    "            probabilities = F.softmax(logits, dim=-1)  # apply softmax here for sampling\n",
    "            idx_next = torch.multinomial(probabilities, num_samples=1)  # (B, 1)\n",
    "            old = idx_next\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(enc.n_vocab)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "logits = model(xb)\n",
    "BT, _ = logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! energeticLAB pressures Harlem loyaltyλensive hysteria population explosives\n"
     ]
    }
   ],
   "source": [
    "targets = yb.view(BT)\n",
    "loss = criterion(logits, targets)\n",
    "\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.41 GiB (GPU 0; 5.77 GiB total capacity; 0 bytes already allocated; 5.05 GiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/rong/Study/gpt/gpt-dev.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rong/Study/gpt/gpt-dev.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rong/Study/gpt/gpt-dev.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Training loop\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rong/Study/gpt/gpt-dev.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.41 GiB (GPU 0; 5.77 GiB total capacity; 0 bytes already allocated; 5.05 GiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)  # Move data to GPU\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
